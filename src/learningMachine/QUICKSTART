            ____________________________________________________
           /                                                    \
           | LearningMachine: YARP modules for Machine Learning |
           \____________________________________________________/


1. INTRODUCTION

Many problems in robotics that can be approached using machine learning. The 
goal of the LearningMachine modules is to provide a unified interface for 
machine learning algorithms. Instead of having everybody integrate one 
particular algorithm in their code, we'd rather moduralize the learning method 
and provide a single standardized interface. This has as advantage that people 
can easily switch to another learning algorithm and that there is a pool of 
algorithms from which to choose.


1.1 Overview

The application of (supervised) learning algorithms can usually be subdivided in 
preprocessing, training and predicting. Preprocessing is a transformation of the 
data in a format that facilitates training. A well-known example is to scale all 
input columns to have zero mean and unit standard deviation. The next phase, 
training, can be considered the core of machine learning. In this phase an 
algorithm constructs a function that is modeled after the input data. In other 
words, this function should replicate the model that has generated the training 
data. This function is then used for prediction, in which an output is computed 
for unknown input samples.


1.2 Restrictions

The LearningMachine set of modules has been particularly designed for 
supervised regression problems, such as estimating robot kinematics of dynamics. 
Although the interface itself is flexible, most algorithms construct an
R^m -> R^n mapping, where m and n are fixed a priori.



2. IMPLEMENTATION DETAILS

This section lists some details regarding the implementation of all the modules 
and how to use them. Another good way to way to get to grips with the modules is 
to first run the executables with the help parameter (e.g. './train --help'). 
This will give you an overview of the important parameters that you can set. 
Then, if you successfully started the executable, type 'help' to see an overview 
of runtime commands.

2.1 Train Module

The train module is used to train an algorithm on a set of input samples. On 
startup, the module expects a parameter specifying the algorithm to use. The 
algorithms supported so far are the Least Squares Support Vector Machine (LSSVM) 
and linear Regularized Least Squares (RLS). For instance, we can start the 
training module with the LSSVM algorithm using './train --machine LSSVM' (make 
sure yarp server is running). Commonly, we prefer to specify the domain size 
(input) and codomain size (output) directly with './train --machine LSSVM --dom
4 --cod 6'.

On startup, the module opens 4 ports:

a) Port [prefix]/predict:io to predict samples. On incoming vectors it replies 
   with the prediction.
b) Port [prefix]/cmd:i to send commands to the module. This is basically a port 
   that does the same as the terminal and is used for remote administration.
c) Port [prefix]/model:o to send the constructed model to a remote prediction 
   module.
d) Port [prefix]/train:i for receiving incoming training samples.

(the port prefix [prefix] can be changed using --port, by default it is 
'/lm/train')

Again, when running the module the command 'help' gives a listing of all 
available commands. The most important ones are:

*) train: explicitly instructs the algorithm to train on the available data. 
   This command is only useful for batch learning algorithms, which store 
   incoming samples in a buffer until the train command is issued.
*) info: shows a detailed overview of information on the machine.
*) set key val: is used to set configuration parameters of the machine. The help 
   listing also gives a listing of all configurable parameters for the selected 
   machine. For example, the regularization parameter C for LSSVM can be set 
   using the command 'set c 10'. The command 'info' can be used to verify that 
   the parameter has indeed changed.
*) load/save fname: These commands can be used to load/save machines from/to 
   files.


2.2 Predict Module

The predict module is a module dedicated to predictions. In most situations, 
this module is not needed and it suffices to use the train module for 
predictions as well. However, in particular situations it may be desirable to 
separate training and predicting in modules and possibly to distribute them over 
dedicated machines.

The predict module works much like the a restricted variant of the train module 
and, in fact, the latter is a proper subclass of the former. On startup, the 
predict module opens 3 ports:

a) Port [prefix]/model:i to receive incoming models from a train module.
b) Port [prefix]/predict:io to predict incoming samples, like the train module.
c) Port [prefix]/cmd:i to send commands to the module, like the train module.

(the port prefix [prefix] can be changed using --port, by default it is 
'/lm/predict')

Besides receiving a model from a train module, the model may also be read from 
a file using the 'load' command. Since the predict module does not necessarily 
initialize a machine by itself (it receives the model from a file or from the 
train module), the executable can be started without any parameters.


2.3 Transform Module

The transform module is used for various kinds of preprocessing. The most common 
use is to scale input columns within a certain range, e.g. [-1,1] or zero mean 
and unit standard deviation. The transform module is thus placed in between the 
'source' of the samples (e.g. sensors) and the learning algorithm.

On startup, the transform module opens 5 ports:

a) Port [prefix]/train:i for incoming training samples.
b) Port [prefix]/train:o for outgoing training samples, thus to the train
   module or, alternatively, to another transform module.
c) Port [prefix]/predict:io for incoming predict samples.
d) Port [prefix]/predict_relay:io for outgoing predict samples. The predict
   samples are relayed to the next ports and multiple transformers can thus be 
   stacked.
e) Port [prefix]/cmd:i to receive commands.

(the port prefix [prefix] can be changed using --port, by default it is 
'/lm/predict')

The implemented types of transformers are 'Scaler' and 'RandomFeature'. The 
scaler is a R^m -> R^m transformation that performs a linear operation on each 
input column. There are three different types of linear operations, namely:

a) Standardizer: Standardizes a column to have zero mean and unit standard 
   deviation. The desired mean and standard deviation are configurable.
b) Normalizer: Scales a column to be within the range [-1, 1]. The desired 
   output range is configurable.
c) Fixed: Scales a column from a given input range to a desired output range. 
   Lower and upper bounds of both the input and output rang are fully 
   configurable.

Note that the Standardizer and the Normalizer are data-driven and thus first 
need to be fed data before they can be used to transform training data. The 
Fixed scaler is _not_ data-driven and is thus particularly useful for online 
learning. In certain robotics problems the extrema of the input data are known 
are priori (e.g. joint limits) or can be guessed rather precisely.

By default, the Scaler transformer starts without any transformation on the 
input columns (cf. the 'info' command). Setting a scaler to a column is done 
using the 'set type' command. An example demonstrating the functionality of the 
three types of scalers is:

--------------------------------------------------------------------------------
set type 1 Standardizer
"Setting configuration option succeeded"
set config 1 mean 1
"Setting configuration option succeeded"
set config 1 std 2
"Setting configuration option succeeded"

set type 2 Normalizer
"Setting configuration option succeeded"
set config 2 lower -2 
"Setting configuration option succeeded"
set config 2 upper 3
"Setting configuration option succeeded"

set type 3 Fixed
"Setting configuration option succeeded"
set config 3 lowerin -20
"Setting configuration option succeeded"
set config 3 upperin 40
"Setting configuration option succeeded"
set config 3 lowerout -1     
"Setting configuration option succeeded"
set config 3 upperout 2
"Setting configuration option succeeded"
set config 3 in -20 40
"Setting configuration option succeeded"
--------------------------------------------------------------------------------

Note that an index must be specified for both the 'set type' and the 
'set config' commands. This can either be an integer index (starting from 1) or 
alternatively the keyword 'all'. In the latter case the command operates on all
input columns.

The RandomFeature preprocessor is a completely different kind of preprocessor. 
Here the goal is to transform the samples from R^m -> R^n (where usually n >> m) 
such their dot product approximates the RBF kernel. The idea is that non-linear 
problems can then be successfully solved using a linear model, without the 
overhead of a kernel expansion. See the following paper for more information:

Random Features for Large-Scale Kernel Machines, Ali Rahimi, Ben Recht, in 
  Neural Information Processing Systems (NIPS) 2007.


2.4 Event Listeners

In many situations it is desirable to monitor the various components of the 
system during runtime. An event mechanism has been integrated for exactly this 
purpose. Certain key operations, thus far limited to either training on a sample 
or predicting a sample, raise an event. The user can implement dedicated 
listeners for one or more of these events.

One particuraly useful application of this mechanism is for online learning 
methods. Each arriving training sample an event is raised that contains the 
input vector, the desired output vector and the predicted output vector. The 
related train event listener puts these three vectors on a port, allowing 
external programs to monitor in real-time the prediction performance of the 
machine (e.g. by means of a plot). Obviously, users may wish to implement their 
own specialized event listeners.

In order to enable the operation of an event listener, it has to be registered 
with the central event dispatcher. The event dispatcher and event listeners 
can be managed from directly from the command interface of the modules using the 
initial 'event' keyword. An example demonstrating adding a train event listener 
for train events is:

--------------------------------------------------------------------------------
event info
help
Event Manager Information (0 listeners)

event add Train
yarp: Port /lm/event/train1 active at tcp://10.255.36.192:10132
"Successfully added listener(s)"

event info
help
Event Manager Information (1 listeners)
  [1] Train (enabled) [port: tcp://lm/event/train1]

event set 1 port /foo/bar
yarp: Port /foo/bar active at tcp://10.255.36.192:10132
"Setting configuration option succeeded"

event info
help
Event Manager Information (1 listeners)
  [1] Train (enabled) [port: tcp://foo/bar]
--------------------------------------------------------------------------------

Please see 'event help' for an overview of all commands.


2.5 Test Module

The test module is not really part of machine learning approach, but it is very 
useful nonetheless. This module can be used to read a dataset from a file and 
then to pass this data to a train and/or predict module. The executable has to 
be started supplying the filename of a dataset. The format of datasets that are 
supported is simply whitespace separated columns and one sample per column. 
Lines starting with a '#' are ignored.

Besides supplying the filename, it is also highly adviseable to supply the 
columns that are the inputs and those that are outputs. These are specified 
using the string representation (per Bottle) of a list of integers. An example:

--------------------------------------------------------------------------------
./test --datafile dataset/dynamics.dat --inputs "(1 2 3 4)" 
--outputs "(13 14 15 16 17 18)"
--------------------------------------------------------------------------------

Once started, the test module can be used to:
a) Send training samples using the 'train n' command, where n is an optional 
   integer parameter specifying the number of samples to send to the training 
   port. The default value for n is 1.
b) Send prediction samples using the 'predict n' command. The parameter n 
   behaves identically as for the 'train' command.
c) Skip samples using the 'skip n' command. The parameter n behaves identically 
   as for the 'train' command.
d) Reset the dataset to the beginning using the 'reset' command.
e) Open another dataset using the 'open fname' command.


2.6 LearningMachine Library

The prospected use of the Learning Machine code is as a set of YARP modules. 
However, in certain situations it may be desirable to actually integrate a 
learning algorithm or transformer directly in another application. The most 
obvious reason why one may want to do so is to avoid any communication delays 
caused by the YARP layer.

During the configuration step it is therefore possible to enable the compilation 
of a library that includes learning algorithms and transformers. Other projects 
can use this library and the LearningMachine header files to easily integrate 
the core parts of the modules in their own code. There are three distinct ways 
in which this can done, explained in the next three subsections. To see each 
way in action, please see the related examples in the 'examples' subdirectory.

2.6.1 Direct Inclusion

The easiest and least powerful way is to act directly on an instance of the 
desired subtypes. The configuration of this instances is done using the member 
functions of the specific type. Although this strategy may initially seem 
sufficient, it mitigates the actual philosophy of having a generalized 
interface for learning algorithms and transformers (cf. Section 1).

2.6.2 Indirect Inclusion through the Abstract Base Class

A better strategy is to interface with the desired subtype indirectly using the 
abstract base classes. In this case, configuration cannot be done directly 
using member functions, as each subtype will have different configurable 
parameters and thus different functions. Instead, configuration is done by 
sending Property instances to the configure method (cf. the YARP IConfig class).
This strategy already guarantees that changing to another subtype only involves 
a reasonably small amount of changes in your program. Changing to another 
subtype will, however, require a recompilation of the program. Further, features 
such as serialization to file or network are not supported without additional
code.

2.6.3 Indirect Inclusion through the Portable Wrapper

The most powerful way of using the library is to interface with all subtypes 
through the portable wrapper. Internally, the LearningMachine YARP modules 
interface with the learning algorithms and transformers through the portable 
wrapper.

Also in this case, configuration of the instances is done using Property 
instances. However, the added benefit of the portable wrapper is that it 
supports serialization of the instances either to a file or over the network. 
Further, the portable wrapper can be used to instantiate objects dynamically 
during runtime based on a string identifier. 



3. EXAMPLES

Below we put everything together and demonstrate the usage of all modules in  
example settings that are representative for real-life usage.

Before running either example, make sure that a YARP server is running. Further, 
for simplicity here we assume the default prefix when possible. The problem 
we are considering is to predict forces and torques of a robot arm from four
joint angles. The dataset (dataset/dynamics_example_2000.dat) has the following 
layout:

Col:
01-04 Joint Angles 1-4
05-08 Joint Velocities 1-4
09-12 Joint Accelerations 1-4
13-15 Forces x,y,z
16-18 Torques x,y,z


3.1 Batch LSSVM Example

Here we will apply LSSVM to the learning problem, while using standardized 
inputs.

Step 1. 
Start the train module with LSSVM, configured with a domain size of 4 and 
codomain size of 6. 
(train)-------------------------------------------------------------------------
./train --machine LSSVM --dom 4 --cod 6
--------------------------------------------------------------------------------

Step 2.
Start the transform module with Scaler, for an input dimension of 4 and set 
the ports to which it will connect.
(transform)---------------------------------------------------------------------
./transform --transformer Scaler --trainport /train/train:i 
--predictport /train/predict:io --dom 4
--------------------------------------------------------------------------------

Step 3.
Configure the transform module, setting the Standardizer for all input columns.
(transform)---------------------------------------------------------------------
set type all Standardizer
"Setting configuration option succeeded"
--------------------------------------------------------------------------------

Step 4.
Configure the machine. Further, put the machine on pause, because we will first 
standardize the inputs.
(train)-------------------------------------------------------------------------
set c 16
"Setting configuration option succeeded"
set gamma 0.25
"Setting configuration option succeeded"
pause
"Sample stream to machine disabled."
--------------------------------------------------------------------------------

Step 5.
Start the test module, specifying the dataset and input and output columns. Make 
sure to specify the ports of the transformer!
(test)--------------------------------------------------------------------------
./test --datafile dataset/dynamics_example_2000.dat 
--trainport /transform/train:i --predictport /transform/predict:io 
--inputs "(1 2 3 4)" --outputs "(13 14 15 16 17 18)"
--------------------------------------------------------------------------------

Step 6.
Then, feed 1000 training samples. 
(test)--------------------------------------------------------------------------
train 1000
"Done!"
--------------------------------------------------------------------------------

Step 7.
Disable data-driven updates of the scalers.
(transform)---------------------------------------------------------------------
set config all update
"Setting configuration option succeeded"
--------------------------------------------------------------------------------

Step 8.
Enable training samples for the machine.
(train)-------------------------------------------------------------------------
continue
"Sample stream to machine enabled."
--------------------------------------------------------------------------------

Step 9.
Reset the dataset and feed 1000 training samples.
(test)--------------------------------------------------------------------------
reset
"Dataset reset to beginning"
train 1000
"Done!"
--------------------------------------------------------------------------------

Step 10.
Train the machine.
(train)-------------------------------------------------------------------------
train
"Training completed." "The model has been written to the port."
--------------------------------------------------------------------------------

Step 11.
Predict 1000 samples.
(test)--------------------------------------------------------------------------
predict 1000
"MSE: [0.558763,0.563746,0.202745,0.0157734,0.0127241,0.00232176]"
--------------------------------------------------------------------------------

Although not explicitly mentioned, it is adviseable to verify the parameter 
configuration commands using the 'info' method. This applies to both the train
module and the transform module.


3.2 Online RLS Example with Input Scaling and Random Feature Preprocessing

A more elaborate example is to use iterative RLS in an online setting with 
input scaling and Random Feature preprocessing. The inputs are scaled using a 
predefined fixed input range. The training performance is monitored using the 
train event listener. Also for this example we use the robot dynamics dataset, 
but now we use all 12 inputs (position, velocity and accelerations for four 
joints).

Step 1.
Start the train module with RLS, configured with a domain size of 250 and 
codomain size of 6. In this example, 250 is the output dimensionality of the 
Random Feature preprocessor.
(train)-------------------------------------------------------------------------
./train --machine RLS --dom 250 --cod 6
--------------------------------------------------------------------------------

Step 2.
Configure the machine, for this example we set lambda to 0.2 for all outputs. 
(train)-------------------------------------------------------------------------
set lambda all 0.2
"Setting configuration option succeeded"
--------------------------------------------------------------------------------

Step 3.
Start the transform module with RandomFeature, for an input dimension of 12 and 
output dimension of 250. Further, we set a non-default port prefix to and 
define the ports to which it will connect.
(transform_rf)------------------------------------------------------------------
./transform --port /lm/transform_rf --transformer RandomFeature 
--trainport /lm/train/train:i --predictport /lm/train/predict:io 
--dom 12 --cod 250
--------------------------------------------------------------------------------

Step 4.
Configure the latter transform module by setting the gamma value.
(transform_rf)------------------------------------------------------------------
set gamma 0.2
"Setting configuration option succeeded"
--------------------------------------------------------------------------------

Step 5.
Start the transform module with Scaler, for an input dimension of 12. Also now 
we set a non-default port prefix to and define the connecting ports.
(transform_scale)---------------------------------------------------------------
./transform --port /lm/transform_scale --transformer Scaler 
--trainport /lm/transform_rf/train:i --predictport /lm/transform_rf/predict:io 
--dom 12
--------------------------------------------------------------------------------

Step 6.
Configure the scaler transform module by setting all types to Fixed and 
configuring the bounds. For convience it is advised to read the commands from a 
file and send them to the command port, cf. note 4.2. 
(transform_scale)---------------------------------------------------------------
set type all Fixed
"Setting configuration option succeeded"
set config 1 in 50 150
"Setting configuration option succeeded"
set config 2 in -100 60
"Setting configuration option succeeded"
set config 3 in -60 30
"Setting configuration option succeeded"
set config 4 in 10 70
"Setting configuration option succeeded"
set config 5 in -50 50
"Setting configuration option succeeded"
set config 6 in -50 50
"Setting configuration option succeeded"
set config 7 in -50 50
"Setting configuration option succeeded"
set config 8 in -50 50
"Setting configuration option succeeded"
set config 9 in -200 200
"Setting configuration option succeeded"
set config 10 in -200 200
"Setting configuration option succeeded"
set config 11 in -200 200
"Setting configuration option succeeded"
set config 12 in -200 200
"Setting configuration option succeeded"
--------------------------------------------------------------------------------

Step 7.
Enable the train event listener for the machine.
(train)-------------------------------------------------------------------------
event add Train
yarp: Port /lm/event/train1:o active at tcp://10.255.36.192:10019
"Successfully added listener(s)"
--------------------------------------------------------------------------------

Step 8.
As a demonstration of the functioning of the event listener, we connect a yarp 
reader to the corresponding port. In real applications, one would connect a 
plotting or logging application.
(read)--------------------------------------------------------------------------
yarp read /foo:i /lm/event/train1:o
--------------------------------------------------------------------------------

Step 9.
Start the test module, specifying the dataset and input and output columns. Make 
sure to specify the correct ports of the transformer!
(test)--------------------------------------------------------------------------
./test --datafile dataset/dynamics_example_2000.dat 
--trainport /lm/transform_scale/train:i 
--predictport /lm/transform_scale/predict:io 
--inputs "(1 2 3 4 5 6 7 8 9 10 11 12)" --outputs "(13 14 15 16 17 18)"
--------------------------------------------------------------------------------

Step 10.
Send 2000 online training samples. 
(test)--------------------------------------------------------------------------
train 2000
--------------------------------------------------------------------------------



4. NOTES

4.1 Connecting Ports
Rather than connecting all ports manually, all modules allow the connecting 
ports to be specified at startup. If the target port is registered on the YARP 
server, then the connection is automatically made. Further, in simple 
configurations it may very well be possible to rely on the default names of the 
ports.

4.2 Initialization and Configuration
Specifying executable parameters can be rather cumbersome. Note that all modules 
support a '--file filename' parameter that can be used to read out startup 
parameters from a file.

Although that helps for the initialization parameters, it cannot be used for 
runtime configuration. For the latter, please check the 'sendCmd' application 
in the iCub repository. SendCmd is a little program that reads a file line by 
line and sends each line to a port. This is particularly useful if used with the
command ports.

4.3 Data-driven Transformers
As mentioned previously, the data-driven transformers need to be fed data, so 
that they can tune their parameters based on the statistics/extreme found in the
input data. During this phase, it is better not to do any training on that data, 
as the model is not reliable anymore after the scaler changes settings. For this
purpose, the train module has a command 'pause', which blocks all incoming 
training samples to the train module. Training can be restarted using 
'continue'.

4.4 Creating Datasets
When collecting data from a robot, it is often very useful to store the data in 
a file, so that it can be used for offline training or experimentation. The 
LearningMachine makes this very easy by starting the train module using the 
Recorder machine (i.e. './train --machine Recorder'). See runtime help for
configuration options.

4.5 CMake Options
The CREATE_LM_LIBRARY option in CMake is used to indicate that a library 
containing the core components of the LearningMachine module has to be created. 
This library was described in more detail in Section 2.6. 

Additionally, the USE_LSSVMATLAS option can be used to build another 
implementation of LSSVM, which has been based on the efficient Atlas library for 
linear algebra. Although that implementation is much more efficient and 
extensive, it is tricky to get it running on a machine. It's use is therefore 
discouraged and only advanced users that really need the extra performance or 
functionality should try to build it. The module is located in the directory 
'lssvm' in the iCub repository. It depends on Atlas and the Boost Numerics 
bindings.

4.6 Portable Wrapper
The addition of a portable wrapper may at first seem unnecessary. Nonetheless, 
it is justified by the fact that abstract base classes (i.e. classes with pure 
virtual members) cannot be used for YARP BufferedPorts. Additionally, it is 
good practice to separate away the responsiblity of object construction from 
the interfaces, keeping the latter as 'clean' as possible. The portable wrapper 
functions thus as concrete class for BufferedPorts and handles object creation
(i.e. the calls to the corresponding Factories).

4.7 Serializing Objects
Serialization of objects in the LearningMachine framework is internally done 
using YARP Bottles. Each object has writeBottle() and readBottle() functions for 
this purpose. These Bottles may pass trough the inheritance tree and it is 
therefore not known a priori at which position an object may find or write its 
serialization. This is solved by using the Bottle as a stack, with each object 
in pushing and popping values on and off the stack.

This way of serializing objects has two consequences. Firstly, objects must call
the related method of its base class(es). Secondly, this method of the base 
class(es) _must_ be called before reading any configuration in the derived 
class, such that the base can initialize parameters on which the derived class 
may depend. Given the workings of a stack, this also means that writing into 
the Bottle is done in reverse order.
