            ____________________________________________________
           /                                                    \
           | LearningMachine: YARP modules for Machine Learning |
           \____________________________________________________/


1. INTRODUCTION

Many problems in robotics that can be approached using machine learning. The
goal of the LearningMachine modules is to provide a unified interface for
machine learning algorithms. Instead of having everybody integrate one
particular algorithm in their code, we'd rather moduralize the learning method
and provide a single standardized interface. This has as advantage that people
can easily switch to another learning algorithm and that there is a pool of
algorithms from which to choose.


1.1 Overview

The application of (supervised) learning algorithms can usually be subdivided in
preprocessing, training and predicting. Preprocessing is a transformation of the
data in a format that facilitates training. A well-known example is to scale all
input columns to have zero mean and unit standard deviation. The next phase,
training, can be considered the core of machine learning. In this phase an
algorithm constructs a function that is modeled after the input data. In other
words, this function should replicate the model that has generated the training
data. This function is then used for prediction, in which an output is computed
for unknown input samples.


1.2 Restrictions

The LearningMachine set of modules has been particularly designed for
supervised regression problems, such as estimating robot kinematics of dynamics.
Although the interface itself is flexible, most algorithms construct an
R^m -> R^n mapping, where m and n are fixed a priori.



2. IMPLEMENTATION DETAILS

This section lists some details regarding the implementation of all the modules
and how to use them. Another good way to way to get to grips with the modules is
to first run the executables with the help parameter (e.g. './train --help').
This will give you an overview of the important parameters that you can set.
Then, if you successfully started the executable, type 'help' to see an overview
of runtime commands.

2.1 Train Module

The train module is used to train an algorithm on a set of input samples. On
startup, the module expects a parameter specifying the algorithm to use. A
listing of the available algorithms can be obtained by specifying the --list
parameter on the command line.
Typically, we can start the training module with the LSSVM algorithm using
'./train --machine LSSVM' (make sure yarp server is running). Commonly, we
prefer to specify the domain size (input) and codomain size (output) directly
with './train --machine LSSVM --dom 4 --cod 6'. If a previously stored machine
is available in a file, it is also possible to directly load this machine using
'./train --load filename'. In that case, additional configuration options are
not passed on to the machine and will need to be configured from the program
prompt.

On startup, the module opens 4 ports:

a) Port [prefix]/predict:io to predict samples. On incoming vectors it replies
   with the prediction.
b) Port [prefix]/cmd:i to send commands to the module. This is basically a port
   that does the same as the terminal and is used for remote administration.
c) Port [prefix]/model:o to send the constructed model to a remote prediction
   module.
d) Port [prefix]/train:i for receiving incoming training samples.

(the port prefix [prefix] can be changed using --port, by default it is
'/lm/train')

Again, when running the module the command 'help' gives a listing of all
available commands. The most important ones are:

*) train: explicitly instructs the algorithm to train on the available data.
   This command is only useful for batch learning algorithms, which store
   incoming samples in a buffer until the train command is issued.
*) info: shows a detailed overview of information on the machine.
*) set key val: is used to set configuration parameters of the machine. The help
   listing also gives a listing of all configurable parameters for the selected
   machine. For example, the regularization parameter C for LSSVM can be set
   using the command 'set c 10'. The command 'info' can be used to verify that
   the parameter has indeed changed.
*) load/save fname: These commands can be used to load/save machines from/to
   files.


2.2 Predict Module

The predict module is a module dedicated to predictions. In most situations,
this module is not needed and it suffices to use the train module for
predictions as well. However, in particular situations it may be desirable to
separate training and predicting in modules and possibly to distribute them over
dedicated machines.

The predict module works much like the a restricted variant of the train module
and, in fact, the latter is a proper subclass of the former. On startup, the
predict module opens 3 ports:

a) Port [prefix]/model:i to receive incoming models from a train module.
b) Port [prefix]/predict:io to predict incoming samples, like the train module.
c) Port [prefix]/cmd:i to send commands to the module, like the train module.

(the port prefix [prefix] can be changed using --port, by default it is
'/lm/predict')

Besides receiving a model from a train module, the model may also be read from
a file using the 'load' command. Since the predict module does not necessarily
initialize a machine by itself (it receives the model from a file or from the
train module), the executable can be started without any parameters.


2.3 Transform Module

The transform module is used for various kinds of preprocessing. The most common
use is to scale input columns within a certain range, e.g. [-1,1] or zero mean
and unit standard deviation. The transform module is thus placed in between the
'source' of the samples (e.g. sensors) and the learning algorithm. Although the
task of the transformer is different from the machine learners described above,
the functionality and interface of the module shares many similarities.

On startup, the transform module opens 5 ports:

a) Port [prefix]/train:i for incoming training samples.
b) Port [prefix]/train:o for outgoing training samples, thus to the train
   module or, alternatively, to another transform module.
c) Port [prefix]/predict:io for incoming predict samples.
d) Port [prefix]/predict_relay:io for outgoing predict samples. The predict
   samples are relayed to the next ports and multiple transformers can thus be
   stacked.
e) Port [prefix]/cmd:i to receive commands.

(the port prefix [prefix] can be changed using --port, by default it is
'/lm/predict')

The implemented types of transformers are 'Scaler' and 'RandomFeature'. The
scaler is a R^m -> R^m transformation that performs a linear operation on each
input column. There are three different types of linear operations, namely:

a) Standardizer: Standardizes a column to have zero mean and unit standard
   deviation. The desired mean and standard deviation are configurable.
b) Normalizer: Scales a column to be within the range [-1, 1]. The desired
   output range is configurable.
c) Fixed: Scales a column from a given input range to a desired output range.
   Lower and upper bounds of both the input and output rang are fully
   configurable.

Note that the Standardizer and the Normalizer are data-driven and thus first
need to be fed data before they can be used to transform training data. The
Fixed scaler is _not_ data-driven and is thus particularly useful for online
learning. In certain robotics problems the extrema of the input data are known
are priori (e.g. joint limits) or can be guessed rather precisely. If desired,
updates of the scalers can be (temporarily) disabled using the 'disable'
command. The following example shows how this is done in detail.

--------------------------------------------------------------------------------
set config 1 disable
"Setting configuration option succeeded"
set config 1 enable
"Setting configuration option succeeded"
--------------------------------------------------------------------------------

By default, the Scaler transformer starts without any transformation on the
input columns using a 'secret' NullScaler (cf. the 'info' command). Setting a
scaler to a column is done using the 'set type' command. An example
demonstrating the functionality of the three types of scalers (or four, if you'd
count the NullScaler as well) is:

--------------------------------------------------------------------------------
set type 1 Standardizer
"Setting configuration option succeeded"
set config 1 mean 1
"Setting configuration option succeeded"
set config 1 std 2
"Setting configuration option succeeded"

set type 2 Normalizer
"Setting configuration option succeeded"
set config 2 lower -2
"Setting configuration option succeeded"
set config 2 upper 3
"Setting configuration option succeeded"

set type 3 Fixed
"Setting configuration option succeeded"
set config 3 lowerin -20
"Setting configuration option succeeded"
set config 3 upperin 40
"Setting configuration option succeeded"
set config 3 lowerout -1
"Setting configuration option succeeded"
set config 3 upperout 2
"Setting configuration option succeeded"
set config 3 in -20 40
"Setting configuration option succeeded"

set type 4 null
"Setting configuration option succeeded"
--------------------------------------------------------------------------------

Note that an index must be specified for both the 'set type' and the
'set config' commands. This can either be an integer index (starting from 1) or
alternatively the keyword 'all'. In the latter case the command operates on all
input columns.


2.4 Event Listeners

In many situations it is desirable to monitor the various components of the
system during runtime. An event mechanism has been integrated for exactly this
purpose. Certain key operations, thus far limited to either training on a sample
or predicting a sample, raise an event. The user can implement dedicated
listeners for one or more of these events.

One particuraly useful application of this mechanism is for online learning
methods. Each arriving training sample an event is raised that contains the
input vector, the desired output vector and the predicted output vector. The
related train event listener puts these three vectors on a port, allowing
external programs to monitor in real-time the prediction performance of the
machine (e.g. by means of a plot). Obviously, users may wish to implement their
own specialized event listeners.

In order to enable the operation of an event listener, it has to be registered
with the central event dispatcher. The event dispatcher and event listeners
can be managed from directly from the command interface of the modules using the
initial 'event' keyword. An example demonstrating adding a train event listener
for train events is:

--------------------------------------------------------------------------------
event info
help
Event Manager Information (0 listeners)

event add Train
yarp: Port /lm/event/train1 active at tcp://10.255.36.192:10132
"Successfully added listener(s)"

event info
help
Event Manager Information (1 listeners)
  [1] Train (enabled) [port: tcp://lm/event/train1]

event set 1 port /foo/bar
yarp: Port /foo/bar active at tcp://10.255.36.192:10132
"Setting configuration option succeeded"

event info
help
Event Manager Information (1 listeners)
  [1] Train (enabled) [port: tcp://foo/bar]
--------------------------------------------------------------------------------

Please see 'event help' for an overview of all commands.


2.5 Test Module

The test module is not really part of machine learning approach, but it is very
useful nonetheless. This module can be used to read a dataset from a file and
then to pass this data to a train and/or predict module. The executable has to
be started supplying the filename of a dataset. The format of datasets that are
supported is simply whitespace separated columns and one sample per column.
Lines starting with a '#' are ignored.

Besides supplying the filename, it is also highly adviseable to supply the
columns that are the inputs and those that are outputs. These are specified
using the string representation (per Bottle) of a list of integers. An example:

--------------------------------------------------------------------------------
./test --datafile dataset/dynamics.dat --inputs "(1 2 3 4)"
--outputs "(13 14 15 16 17 18)"
--------------------------------------------------------------------------------

Once started, the test module can be used to:
a) Send training samples using the 'train n' command, where n is an optional
   integer parameter specifying the number of samples to send to the training
   port. The default value for n is 1.
b) Send prediction samples using the 'predict n' command. The parameter n
   behaves identically as for the 'train' command.
c) Skip samples using the 'skip n' command. The parameter n behaves identically
   as for the 'train' command.
d) Explicitly send a single sample using the 'shoot' command. The parameter is
   the input sample in a Bottle format. This command is particularly useful for
   debugging.
e) Reset the dataset to the beginning using the 'reset' command.
f) Open another dataset using the 'open fname' command.


2.6 LearningMachine Library

The prospected use of the Learning Machine code is as a set of YARP modules.
However, in certain situations it may be desirable to actually integrate a
learning algorithm or transformer directly in another application. The most
obvious reason why one may want to do so is to avoid any communication delays
caused by the YARP layer.

During the configuration step it is therefore possible to enable the compilation
of a library that includes learning algorithms and transformers. Other projects
can use this library and the LearningMachine header files to easily integrate
the core parts of the modules in their own code. There are three distinct ways
in which this can done, explained in the next three subsections. To see each
way in action, please see the related examples in the 'examples' subdirectory.

2.6.1 Direct Inclusion

The easiest and least powerful way is to act directly on an instance of the
desired subtypes. The configuration of this instances is done using the member
functions of the specific type. Although this strategy may initially seem
sufficient, it mitigates the actual philosophy of having a generalized
interface for learning algorithms and transformers (cf. Section 1).

2.6.2 Indirect Inclusion through the Abstract Base Class

A better strategy is to interface with the desired subtype indirectly using the
abstract base classes. In this case, configuration cannot be done directly
using member functions, as each subtype will have different configurable
parameters and thus different functions. Instead, configuration is done by
sending Property instances to the configure method (cf. the YARP IConfig class).
This strategy already guarantees that changing to another subtype only involves
a reasonably small amount of changes in your program. Changing to another
subtype will, however, require a recompilation of the program. Further, features
such as serialization to file or network are not supported without additional
code.

2.6.3 Indirect Inclusion through the Portable Wrapper

The most powerful way of using the library is to interface with all subtypes
through the portable wrapper. Internally, the LearningMachine YARP modules
interface with the learning algorithms and transformers through the portable
wrapper.

Also in this case, configuration of the instances is done using Property
instances. However, the added benefit of the portable wrapper is that it
supports serialization of the instances either to a file or over the network.
Further, the portable wrapper can be used to instantiate objects dynamically
during runtime based on a string identifier.



3. EXAMPLES

Below we put everything together and demonstrate the usage of all modules in
example settings that are representative for real-life usage.

Before running either example, make sure that a YARP server is running. Further,
for simplicity here we assume the default prefix when possible. The problem
we are considering is to predict forces and torques of a robot arm from four
joint angles. The dataset (exampledata/icubdyn_train.dat) has the following
layout:

Col:
01-04 Joint Angles 1-4
05-08 Joint Velocities 1-4
09-12 Joint Accelerations 1-4
13-15 Forces x,y,z
16-18 Torques x,y,z


3.1 Batch LSSVM Example

Here we will apply LSSVM to the learning problem, while using standardized
inputs.

Step 1.
Start the train module with LSSVM, configured with a domain size of 4 and
codomain size of 6.
(train)-------------------------------------------------------------------------
./train --machine LSSVM --dom 4 --cod 6
--------------------------------------------------------------------------------

Step 2.
Start the transform module with Scaler, for an input dimension of 4 and set
the ports to which it will connect.
(transform)---------------------------------------------------------------------
./transform --transformer Scaler --trainport /lm/train/train:i
--predictport /lm/train/predict:io --dom 4
--------------------------------------------------------------------------------

Step 3.
Configure the transform module, setting the Standardizer for all input columns.
(transform)---------------------------------------------------------------------
set type all Standardizer
"Setting configuration option succeeded"
--------------------------------------------------------------------------------

Step 4.
Configure the machine. Further, put the machine on pause, because we will first
standardize the inputs.
(train)-------------------------------------------------------------------------
set c 16
"Setting configuration option succeeded"
set gamma 0.25
"Setting configuration option succeeded"
pause
"Sample stream to machine disabled."
--------------------------------------------------------------------------------

Step 5.
Start the test module, specifying the dataset and input and output columns. Make
sure to specify the ports of the transformer!
(test)--------------------------------------------------------------------------
./test --datafile exampledata/icubdyn_train.dat
--trainport /lm/transform/train:i --predictport /lm/transform/predict:io
--inputs "(1 2 3 4)" --outputs "(13 14 15 16 17 18)"
--------------------------------------------------------------------------------

Step 6.
Then, feed 1000 training samples.
(test)--------------------------------------------------------------------------
train 1000
"Done!"
--------------------------------------------------------------------------------

Step 7.
Disable data-driven updates of the scalers.
(transform)---------------------------------------------------------------------
set config all disable
"Setting configuration option succeeded"
--------------------------------------------------------------------------------

Step 8.
Enable training samples for the machine.
(train)-------------------------------------------------------------------------
continue
"Sample stream to machine enabled."
--------------------------------------------------------------------------------

Step 9.
Reset the dataset and feed 1000 training samples.
(test)--------------------------------------------------------------------------
reset
"Dataset reset to beginning"
train 1000
"Done!"
--------------------------------------------------------------------------------

Step 10.
Train the machine.
(train)-------------------------------------------------------------------------
train
"Training completed." "The model has been written to the port."
--------------------------------------------------------------------------------

Step 11.
Predict 1000 samples.
(test)--------------------------------------------------------------------------
predict 1000
"MSE: [..., ..., ...]"
--------------------------------------------------------------------------------

Although not explicitly mentioned, it is adviseable to verify the parameter
configuration commands using the 'info' method. This applies to both the train
module and the transform module.


3.2 Online RLS Example with Input Scaling and Random Feature Preprocessing

A more elaborate example is to use iterative RLS in an online setting with
input scaling and Random Feature preprocessing. The inputs are scaled using a
predefined fixed input range. The training performance is monitored using the
train event listener. Also for this example we use the robot dynamics dataset,
but now we use all 12 inputs (position, velocity and accelerations for four
joints).

Step 1.
Start the train module with RLS, configured with a domain size of 250 and
codomain size of 6. In this example, 250 is the output dimensionality of the
Random Feature preprocessor.
(train)-------------------------------------------------------------------------
./train --machine RLS --dom 250 --cod 6
--------------------------------------------------------------------------------

Step 2.
Configure the machine, for this example we set lambda to 0.2 for all outputs.
(train)-------------------------------------------------------------------------
set lambda 0.2
"Setting configuration option succeeded"
--------------------------------------------------------------------------------

Step 3.
Start the transform module with RandomFeature, for an input dimension of 12 and
output dimension of 250. Further, we set a non-default port prefix to and
define the ports to which it will connect.
(transform_rf)------------------------------------------------------------------
./transform --port /lm/transform_rf --transformer RandomFeature
--trainport /lm/train/train:i --predictport /lm/train/predict:io
--dom 12 --cod 250
--------------------------------------------------------------------------------

Step 4.
Configure the latter transform module by setting the gamma value.
(transform_rf)------------------------------------------------------------------
set gamma 0.2
"Setting configuration option succeeded"
--------------------------------------------------------------------------------

Step 5.
Start the transform module with Scaler, for an input dimension of 12. Also now
we set a non-default port prefix to and define the connecting ports.
(transform_scale)---------------------------------------------------------------
./transform --port /lm/transform_scale --transformer Scaler
--trainport /lm/transform_rf/train:i --predictport /lm/transform_rf/predict:io
--dom 12
--------------------------------------------------------------------------------

Step 6.
Configure the scaler transform module by setting all types to Fixed and
configuring the bounds. For convience it is advised to read the commands from a
file and send them to the command port, cf. note 4.2.
(transform_scale)---------------------------------------------------------------
set type all Fixed
"Setting configuration option succeeded"
set config 1 in 50 150
"Setting configuration option succeeded"
set config 2 in -100 60
"Setting configuration option succeeded"
set config 3 in -60 30
"Setting configuration option succeeded"
set config 4 in 10 70
"Setting configuration option succeeded"
set config 5 in -50 50
"Setting configuration option succeeded"
set config 6 in -50 50
"Setting configuration option succeeded"
set config 7 in -50 50
"Setting configuration option succeeded"
set config 8 in -50 50
"Setting configuration option succeeded"
set config 9 in -200 200
"Setting configuration option succeeded"
set config 10 in -200 200
"Setting configuration option succeeded"
set config 11 in -200 200
"Setting configuration option succeeded"
set config 12 in -200 200
"Setting configuration option succeeded"
--------------------------------------------------------------------------------

Step 7.
Enable the train event listener for the machine.
(train)-------------------------------------------------------------------------
event add Train
yarp: Port /lm/event/train1:o active at tcp://10.255.36.192:10019
"Successfully added listener(s)"
--------------------------------------------------------------------------------

Step 8.
As a demonstration of the functioning of the event listener, we connect a yarp
reader to the corresponding port. In real applications, one would connect a
plotting or logging application.
(read)--------------------------------------------------------------------------
yarp read /foo:i /lm/event/train1:o
--------------------------------------------------------------------------------

Step 9.
Start the test module, specifying the dataset and input and output columns. Make
sure to specify the correct ports of the transformer!
(test)--------------------------------------------------------------------------
./test --datafile exampledata/icubdyn_train.dat
--trainport /lm/transform_scale/train:i
--predictport /lm/transform_scale/predict:io
--inputs "(1 2 3 4 5 6 7 8 9 10 11 12)" --outputs "(13 14 15 16 17 18)"
--------------------------------------------------------------------------------

Step 10.
Send 2000 online training samples.
(test)--------------------------------------------------------------------------
train 2000
--------------------------------------------------------------------------------



4. NOTES

4.1 Connecting Ports
Rather than connecting all ports manually, all modules allow the connecting
ports to be specified at startup. If the target port is registered on the YARP
server, then the connection is automatically made. Further, in simple
configurations it may very well be possible to rely on the default names of the
ports.

4.2 Initialization and Configuration
Specifying executable parameters can be rather cumbersome. Note that all modules
support a '--commands filename' parameter that can be used to read out startup
parameters from a file.

Although that helps for the initialization parameters, it cannot be used for
runtime configuration. For the latter, please check the 'sendCmd' application
in the iCub repository. SendCmd is a little program that reads a file line by
line and sends each line to a port. This is particularly useful if used with the
command ports.

4.3 Data-driven Transformers
As mentioned previously, the data-driven transformers need to be fed data, so
that they can tune their parameters based on the statistics/extreme found in the
input data. During this phase, it is better not to do any training on that data,
as the model is not reliable anymore after the scaler changes settings. For this
purpose, the train module has a command 'pause', which blocks all incoming
training samples to the train module. Training can be restarted using
'continue'.

4.4 Creating Datasets
When collecting data from a robot, it is often very useful to store the data in
a file, so that it can be used for offline training or experimentation. The
LearningMachine makes this very easy by starting the train module using the
Recorder machine (i.e. './train --machine Recorder'). See runtime help for
configuration options.

4.5 Prediction Object
The result of a prediction is contained in a Prediction object, which always
contains both the prediction of the expected value and optionally also a
predictive variance (measured as the unit standard deviation). Prediction
objects can be read directly from the port, however, in this case it is
required to include the corresponding header file (linking against the library
is *not* required).
Alternatively, the Prediction object is compatible with the standard Yarp
objects, namely PortablePair<Vector,Vector>. It is therefore possible to avoid
including any learningMachine headers by reading this object from the port. The
expected value prediction will reside in the head of the PortablePair, while the
optional predictive variance is in the body (see PortablePair documentation for
more details). Note that when the predictive variance is not available, this is
indicated by an empty vector.
Given the compatibility of PortablePair with standard Bottles, it is also
possible to read the result as a Bottle instance. In this case, the top-level
Bottle contains a list of two nested Bottles, one for the expected value
prediction and another for the predictive variance.

4.6 Portable Wrapper
The addition of a portable wrapper may at first seem unnecessary. Nonetheless,
it is justified by the fact that abstract base classes (i.e. classes with pure
virtual members) cannot be used for YARP BufferedPorts. Additionally, it is
good practice to separate away the responsiblity of object construction from
the interfaces, keeping the latter as 'clean' as possible. The portable wrapper
functions thus as concrete class for BufferedPorts and handles object creation
(i.e. the calls to the corresponding Factories).

4.7 Serializing Objects
Serialization of objects in the LearningMachine framework is internally done
using YARP Bottles. Each object has writeBottle() and readBottle() functions for
this purpose. These Bottles may pass trough the inheritance tree and it is
therefore not known a priori at which position an object may find or write its
serialization. This is solved by using the Bottle as a stack, with each object
in pushing and popping values on and off the stack.

This way of serializing objects has two consequences. Firstly, objects must call
the related method of its base class(es). Secondly, this method of the base
class(es) _must_ be called before reading any configuration in the derived
class, such that the base can initialize parameters on which the derived class
may depend. Given the workings of a stack, this also means that writing into
the Bottle is done in reverse order.
